
# 12

[Python] Cython 让代码快100倍 https://www.bilibili.com/video/BV1JQiCY9ELX/
- 回复：
  * > 实际上是非常难用的。***一般你要优化的计算都是在某个数据结构上的计算，我们姑且就不谈或者说假设能避免几种数据结构来回转换这种恶臭场景吧，也就是说你只需要维护一次数据结构在两种语言间的转换，这也已经很恶心了只要你写过的话***。然后呢，这个优化做完后相当于你把原来被优化沾上的许多代码至少是关键类给封印了，因为你绝对不希望自己因为一点小小的修改和新优化就又要打开那个恶心的cpy文件。总之，除了算法优化之外，能用numpy以及相关的库就用numpy，或者是把核心、耗时的计算给inplace化或者至少是多写一个inplace版本，这样大概率是通常的最佳选择。
    >> 速度只是其中一个好处，另外一个是反编译困难。因为特意预编译的这部分代码都是十分珍贵的，不可能直接放py给大伙看吧？
    >>> 有道理，不过代码混淆应该有更成熟的做法，借助这个有点太胡闹了吧。。
    >>>> 我一般都是把python的文件每个都编译成.so文件，最后只有代码入口是python文件来防止别人看我的代码。但是视频里这种循环操作都是用C++写再写python binding，而且还能做内存管理防止数据被多次复制，这种cython的写法只用过一次，因为实在是没办法去做并行化，但是这种其实直接用`cupy`来替换`numpy`速度也很快，***反正开发过程中走下来，就cython是最恶心的***
  * > 我初学cython想用它 感觉最大的阻力在我想优化的项目是大csv文件的文本处理 然后string这块好像转成char*之后就不好处理了 想用cython编译c++又发现string得先encode为bytes才能自动转c++string 一通操作比不搞还慢 这可咋整[大哭][大哭][大哭]
    >> 大csv直接`duckdb`或者`pyarrow`
    >>> 可以试一下`polars`库 向量处理一列 我1.5g的数据处理只要二十秒左右
    >>>> csv太大的话可以转成parquet，速度比csv快很多，就是轮子没有pandas那么丰富
  * > 大佬我今天看你的代码感受颇深！但有个问题，我大部分的计算都是用过pandas和numpy来运行的，这种可以用cython编译么？还有就是还有存储和调用文件，这种cython会帮我处理么？谢谢！
    >> ***pandas和numpy这种库，底层已经是高效的 C 实现，直接用 Cython 编译效果可能有限***。如果是自定义的计算逻辑，比如`循环`或`复杂的数学公式`，那就很适合用Cython来优化。 <br> ***至于存储和调用文件，Cython本身是专注于代码运行效率的，不会特别帮你优化文件操作***。不过，你可以把涉及到复杂计算的部分用Cython加速，文件读写用 Python 原生方式实现，分工明确，效果更佳。
  * > 用到最后还是`numpy`+`numba`，上手简单好调试
  * > 试试`numba`的jit加速效果[打call]
    >> 很多库不廉容numba的jit，在numpy也只支持一部分类型与函数，真用jit，还是pypy吧，可惜pypy的scipy的win版本没出，不然我就转pypy了，现在最稳妥的加速还是cython[笑哭]
    >>> numba和cython效率没有实质性区别，而jit的最大问题是在写入内存时花费的时间，如果代码多，会得不偿失，并且兼容性问题很大，有那功夫不如用cython
    >>>> 不建议使用numba，他只能优化部分数学计算，如果需要计算可以考虑numpy
