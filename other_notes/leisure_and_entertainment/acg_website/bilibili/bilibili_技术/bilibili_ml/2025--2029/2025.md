
# 12

【闪客】你管这破玩意叫智能体？Manus背后的技术 https://www.bilibili.com/video/BV1PRvhBSEwx/
- `6:35`: 关于agent的主程序，你可以参考这个开源项目： https://github.com/MarkTechStation/VideoCode
- 参考链接：
  * https://github.com/MarkTechStation/VideoCode
    + BiliBili: https://space.bilibili.com/1815948385
    + 小红书: https://www.xiaohongshu.com/user/profile/64687d3a000000002901216f

# 11

【闪客】它是深度学习的核心，但却被起了个烂名字，十分钟彻底搞懂张量！ https://www.bilibili.com/video/BV1SB2gBFEyu/  【`11.7`】
- 回复：
  * > 推荐一下张量的彭罗斯记号，用连线图来清楚地表示各个张量之间乘积与求和。简介资料可以参考The Tensor Cookbook (｀・ω・´)

# 07

【[ :star: ][`*`]】 一小时从函数到Transformer！一路大白话彻底理解AI原理 https://www.bilibili.com/video/BV1NCgVzoEG9/  【`7.21`】
- 回复：
  * > 有时候真觉得这像玄学。。。采用什么激活函数，多少层，损失函数，怎么来都可以做，效果好不好看训练结果[笑哭]（我学的也不好，理解就这样了）
    >> 本来就是黑箱，内部什么情况不知道，随机结果
    >>> 挺玄，但是事后用数学解释又解释得通，可以看苏神的博客 `kexue.fm`
  * > 核心要点（中文归纳）
    ```console
    1. 目标：
    找到参数 w、b，使神经网络（本质是线性变换+激活函数的组合）输出尽可能逼近真实数据。
    2. 如何衡量“好”：
    定义损失函数 L = 均方误差（MSE），即所有样本预测值与真实值差的平方和的平均值；L 越小，拟合越好。
    3. 简单线性回归的解法：
    对单变量 w 的二次损失函数求导并令导数为 0，可直接解析得到最优 w；若同时含 w 和 b，则需令两个偏导数为 0，对应三维“碗状”曲面最低点。
    4. 神经网络无法解析求解：
    多层非线性导致损失函数复杂，无法一步求导得解，只能“一点点试”。
    5. 梯度下降法：
    • 计算损失函数对各参数的偏导数（梯度）。
    • 沿梯度反方向按学习率 α 更新参数，使 L 逐步减小。
    • 迭代直至 L 足够小。
    6. 反向传播（链式法则）：
    • 先“前向传播”得预测值并算 L。
    • 再从输出层开始，利用链式法则把梯度逐层向左“反向传播”，高效求得所有参数的偏导。
    • 每轮训练 = 前向传播 + 反向传播 + 参数更新。
    7. 训练循环：
    重复上述过程多轮，参数逐渐收敛，最终得到能较好拟合真实数据的网络。
    ```
  * > Softmax是啥呀好像没听到讲。btw，啥时候能线下跟up合影！
    >> 假设你有10个类别，你最后通过一个全连接层将你的输出转化为1维向量，你想将它们转化为概率，但是向量里的10个值有正有负，这显然没法作为概率对吧。 <br> 所以很自然想到了指数函数，它在整个定义域内都是正的，将向量里每个值都经过指数函数后，你就得到了一个全为正数的向量，再将每个值除以10个值的总和，就得到了它们的占比，也就是概率，而10个值概率之和也恰好等于1，这就是softmax。(maybe你看到评论的时候早已经学会了[脱单doge])
  * > up视频中，可以随意添加/修改网络层，还能查看简单训练效果的界面是什么呀，感觉对学习神经网络非常有用，把网络直观实现出来了[星星眼][星星眼][星星眼]，能不能分享一下@飞天闪客
    >> playground.tensorflow.org
  * > @飞天闪客 个人觉得，残差网络还是需要讲的。虽然这个技巧很简单，但影响非常大，还能够把不同的网络结构（比如ResNet和UNet）串起来。
  * > 为什么卷积运算可以简化计算？卷积完不还是相同像素数量的图片吗
    >> 卷积计算是局部连接的，不是那种全连接层这种繁琐的计算，而且支持权重共享，所以相对来说计算简化很多
  * > 卷积层可以理解为有一个作用是减少参数量吗
    >> 可以吧

# 02

从函数到神经网络【白话DeepSeek01】 https://www.bilibili.com/video/BV1uGA3eLEeu/  【`2.16`】
