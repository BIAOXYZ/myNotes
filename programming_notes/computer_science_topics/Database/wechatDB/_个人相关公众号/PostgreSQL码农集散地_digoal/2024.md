
# 2024.06

第100期吐槽:PG数据库被注水了?孤儿文件的形成、查找和清理 https://mp.weixin.qq.com/s/N-NCa9pVoQde5EcuFqiP_g
- > **孤儿文件的产生**
- > **复现方法**

第31期吐槽:PG 不支持分区索引，给大表埋下巨大隐患 https://mp.weixin.qq.com/s/qvWebpLOu9rc7kKneII-Zw
- > 1、产品的问题点
  * > PG 不支持分区索引
  * > 之前写过一篇单表不要超过8.9亿。《[什么？PG单表别超过8.9亿条记录?](https://mp.weixin.qq.com/s/M07Q9dKcEgqjxW31YhXM8A) 》虽然17解决了那个问题，今天我要再放一个理由，总之PG大表支持就是差点意思。
- > 2、问题点背后涉及的技术原理
  * > PG的索引支持到表级别, 如果是分区表那么每个分区创建对应索引, 索引不能单独进行分区. 例如一个100亿条记录的单表, 如果要创建索引只能创建普通索引, ***索引本身不能分区***.
  * > 或者如果这是个分区表，PG只能创建每个分区的本地索引，不能创建分区表索引分区的统一全局索引并对索引任意选择列进行分区。
- > 3、这个问题将影响哪些行业以及业务场景
  * > 当单表较大时
  * > 当使用分区表时
- > 4、会导致什么问题?
  * > 单表较大时, 索引也会非常庞大, 可能带来一些问题:
    + > 创建索引的耗时变长,
    + > 索引深度变大导致搜索路径变深, 需要访问更多的数据块才能访问到索引的leaf node, 性能下降,
    + > 单个索引巨大，无法并行回收垃圾，垃圾回收时间变长, 更容易引发数据膨胀, 性能逐渐降低.
    + > 如果系统中有多个块设备、多个表空间, 由于1个索引只能存放在1个表空间内, 那么将无法很好的利用多个块设备的性能.
- > 5、业务上应该如何避免这个坑
  * > 避免单表(或单一分区)的数据量过大, 例如不超过`8.9亿条记录`(经验). 超过后建议分区.如果更新频率巨大, 例如建议单个分区不超过`1亿条`(经验)

【[:star:][`*`]】 性能遥遥领先？我要曝光一些数据库“作弊”手段 https://mp.weixin.qq.com/s/OUA_cnmbyZnnRW0z0mw44A
- > 既然是作弊, 有一些技巧有丢数据库风险/数据库不一致风险的, 有一些属于耍流氓, 所以千万不要让对手知道, 否则被打0分可别怪我. 你有坚硬的商务关系当我没说, 算我"助纣为虐"了.
- > **一、TPCH**
  * > 要大幅度提升OLTP数据库的tpch性能, 这不是扯淡吗? 改造优化器、列存储、向量化是必须滴, 不过PG还真可以作弊, 例如:
    ```console
    1、把数据库装进parquet
    2、把结构装进duckdb
    3、把tpch的22条query做成视图
    4、用fdw调用duckdb
    ```
    > 搞定, 一气呵成, 大概能**提升个40几倍的性能吧**. 别让人知道, 你这是在耍流氓.
- > **二、高并发小事务**
  * > 1、配置较大的shared buffer.
  * > 2、高并发, 就PG这种进程模型数据库, 我都吐槽过无数遍, 并发连接上千后性能急剧下降, 早晚得上内置连接池. 外置连接池增加了1跳, 而且外置连接池很难做到无缝迁移事务和会话变量, 限制较多. 第一优先推荐使用内置连接池, 参考:
  * > 3、拉长 checkpoint 周期, 可以配置:
    ```console
    #checkpoint_timeout = 1d             # range 30s-1d  
    max_wal_size = 1GB  
    min_wal_size = 64GB  
    #checkpoint_flush_after =          # measured in pages, 0 disables
    ```
  * > 4、拉长 checkpoint_completion_target , 可以配置:
    ```console
    #checkpoint_completion_target = 0.9     # checkpoint target duration, 0.0 - 1.0
    ```
  * > 5、观察pg_catalog.pg_stat_bgwriter, 尽量减少buffers_backend_fsync, 调整bgwriter的工作量和间歇, 尽量让bgwriter去淘汰脏页, 可以配置:
    ```console
    #bgwriter_delay = 10ms                 # 10-10000ms between rounds  
    #bgwriter_lru_maxpages = 500            # max buffers written/round, 0 disables  
    #bgwriter_lru_multiplier = 2.0          # 0-10.0 multiplier on buffers scanned/round  
    #bgwriter_flush_after = 512kB           # measured in pages, 0 disables
    ```
  * > 6、关闭hint和checksum, 降低CPU和datafile, wal日志量.
    ```console
    #wal_log_hints = off                    # also do full page writes of non-critical updates  
                                            # (change requires restart)
    ```
  * > 以上都属于君子配置, 没什么危害. 下面来一点作弊配置.
  * > 1、少写点wal.
    ```console
    #wal_level = minimal                    # minimal
    ```
  * > 2、关闭 wal同步提交, 小事务写性能飙升.
    ```console
    synchronous_commit = off
    ```
    > 数据库崩溃会丢数据, 但是不会导致数据不一致. 丢多少则取决于以下配置:
    ```console
    #wal_writer_delay = 200ms               # 1-10000 milliseconds  
    #wal_writer_flush_after = 1MB           # measured in pages, 0 disables
    ```
  * > 3、关闭fpw, 以确保检查点期间性能丝滑. 如果你的文件系统是cow的, 大胆关闭fpw没有任何危害.
    ```console
    #full_page_writes = off
    ```
    > 如果文件系统不是cow的, 关闭后可能导致坏块隐患. 参考阅读:《一起学PolarDB - 第2期 - 为什么FPW是以牺牲(性能、存储空间、稳定性)换取的可靠性?》《DB吐槽大会,第11期 - FPW | Double Write》
  * > 4、关闭fsync, 高度危险参数, 相当于写IO全部异步了, 把压力给到OS刷脏. 带来的后果是数据库可能有丢数据、坏块等风险. 但是写性能会急剧提升.
    ```console
    #fsync = on                             # flush data to disk for crash safety  
                                            # (turning this off can cause  
                                            # unrecoverable data corruption)
    ```
- > **三、批量导入**
  ```console
  1、使用最大的block size.
  2、使用unlogged table
  3、关闭全局或被导入表的autovacuum
  4、删除被导入表上不必要的索引
  5、批量导入, 例如使用copy导入、使用insert into table values (),(),()...();
  6、使用pg_bulkload工具导入, 这个工具导入也是不写wal日志的.
  7、先把数据文件生成, 再导入. 这个解决的是block extent lock瓶颈.
  ```
  > 例如要导入1000万记录, 先导入进去, 然后删除除了最后一个数据块里的一条记录的其他所有记录, 然后vacuum这个表, 这样即使vacuum, 也不会回收物理空间, 而是把所有page都清空为可插入状态, 再次导入时就不需要分配block了.
  > 
  > 参考我的吐槽信息:《DB吐槽大会,第28期 - PG 每次只扩展1个block》
- > **以上“作弊手段”, 学会了吗? 如果你PK赢了一定要来打赏一下哟.**

第25期吐槽:PG的物理Standby无法Partial导致单元化架构/SaaS使用不灵活 https://mp.weixin.qq.com/s/xb4PWZQbi9EqS-cP3XZzug

第22期吐槽:DB容灾节点延迟了，网络带宽瓶颈？用CPU换啊！该“魔法”PG还不支持！
- > 1、产品的问题点
  * > PG 不支持libpq协议层压缩
- > 2、问题点背后涉及的技术原理
  * > libpq是PG 客户端基础驱动, 客户端与数据库交互的信息流不支持压缩传输
- > 3、这个问题将影响哪些行业以及业务场景
  * > 网络带宽或延迟成为瓶颈的场景, 例如广域网链路的数据导入、导出备份、容灾实例。用过海外的服务器你就知道带宽资源有多宝贵。
  * > 写入量、查询返回记录较多的业务, 例如IOT, 时序类.
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内核层支持libpq压缩 , 可能快了, 请看这个patch: https://www.postgresql.org/message-id/flat/CAOYmi%2Bnv5cndsU5XEvZdDMkfuqC5uG0MtQ%2B5w8GS8d-FX0yaXQ%40mail.gmail.com#8b0b5839390370fcd86bc3677bed87e9

性能小金刚 DuckDB 正式发布 v1.0.0, 主打"稳定性" https://mp.weixin.qq.com/s/vJfsIyoFxAU8pj5CoSi54A
- > 原文: Announcing DuckDB 1.0.0 https://duckdb.org/2024/06/03/announcing-duckdb-100.html
- > **为什么是现在宣布 v1.0.0 正式发布？**
  * > 发布v1.0.0的主要障碍之一是存储格式。DuckDB有自己的定制数据存储格式。这种格式允许用户在单个文件中管理许多（可能非常大的）表，具有完整的事务语义和最先进的压缩。当然，设计新的文件格式并非没有挑战，随着时间的推移，我们必须对格式进行重大更改。这导致了次优情况，即每当发布新的DuckDB版本时，使用旧版本创建的文件无法与新的DuckDB版本配合使用，必须手动升级。这个问题早在2月份的v0.10.0中就得到了解决——我们引入了DuckDB存储格式的向后兼容性和有限的前向兼容性。
  * > 虽然DuckDB官方说发布v1.0.0的主要障碍之一是存储格式, 但是我相信大多数人会选择使用外部parquet存储, 对于DuckDB内部的存储仅用于存储元数据, 例如Parquet文件的视图.
- > **"稳定性"是 DuckDB v1.0.0 的主要标签**
  * > 我们观察到DuckDB的使用量和广度惊人地增长，并且没有看到严重问题的报道有所增加。与此同时，有数千个测试用例，每天晚上运行数百万个测试查询。我们运行大量微基准和标准化基准套件，以发现性能回归。DuckDB不断受到各种`模糊器`的折磨，这些`模糊器`构建各种野生SQL查询，以确保我们不会错过奇怪的角落情况。总而言之，这为我们建立了发布v1.0.0的必要信心。 || We’ve observed the frankly staggering growth in the amount and breadth of use of DuckDB in the wild, and have not seen an increase in serious issues being reported. Meanwhile, there are thousands of test cases with millions of test queries being run every night. We run loads of microbenchmarks and standardized benchmark suites to spot performance regressions. DuckDB is constantly being tortured by various `fuzzers` that construct all manners of wild SQL queries to make sure we don’t miss weird corner cases. All told, this has built the necessary confidence in us to release a 1.0.0.
    >> //notes：有点好奇这些 fuzzer 是怎么构建的？
      ```sh
      # 个人补充（from chatgpt）

      1. 流行的模糊测试工具
      - AFL (American Fuzzy Lop)：一个广泛使用的 Fuzzer，以其通过变异输入数据发现错误的有效性著称。
      - LibFuzzer：LLVM 项目的一个进程内、覆盖引导的进化模糊测试引擎。
      - SQLsmith：一个专门用于 SQL 数据库的 Fuzzer，通过生成随机 SQL 查询测试 SQL 解析器和引擎的鲁棒性。
      ```
- > 和我对PG社区的判断一样, 我认为PG
  * > 一方面要做好一个数据库产品最基本的内核能力(安全、稳定、可靠、体验), 从而让更多用户敢把它用在核心系统业务中.
  * > 另一方面要做好可扩展的接口(***例如 wire protocol, hook, fdw, tam, pllanguage, type, udf, op, index 等***), 让更多的外围开发者可以参与其中, 使得PG的生态蓬勃发展, 保持PG的生命力和创新性.

第21期吐槽:90%的性能抖动是缺少这个功能造成的！也是DBA害怕开发去线上跑SQL的魔咒 https://mp.weixin.qq.com/s/mA8_QAoLfrFz_My6i1ls1g
- > 1、产品的问题点
  * > PG 没有持久化Shared Buffer
- > 2、问题点背后涉及的技术原理
  * > PG Shared Buffer采用1个大池子(除了ring buffer以外), 数据访问和修改需要先将数据写入shared buffer, 当内存不够时使用LRU算法淘汰shared buffer中的page.
  * > pG也设计了ring buffer来降低扫描大量数据的任务（例如大表全表扫描，建索引，垃圾回收，analyze等任务）引起的热表buffer被挤出的概率。但它有边界，***我没记错的话全表扫描时表大小要超过shared buffer 四分之一才会触发使用ring buffer具体见代码***。
- > 4、会导致什么问题?
  * > 一些突发的大的查询可能将热数据挤出shared buffer, 从而影响热数据相关SQL的响应速度, 导致性能抖动, 体验下降.
  * > 开发出于调试或紧急事件需要线上跑SQL, 可能把热表挤出去。导致高峰期性能抖动。

第20期吐槽:PG17新版本这么香，为什么不升级呢？居然是因为这个 https://mp.weixin.qq.com/s/WkCpmGkG8FXemUBEQH1j1A
- > 1、产品的问题点
  * > pg_upgrade可以通过迁移元数据来支持大版本升级, 但是不支持增量数据.
- > 6、业务上避免这个坑牺牲了什么, 会引入什么新的问题
  * > pglogical有前置依赖
    + > 逻辑复制的表必须有PK和UK, 没有PK和UK的表要开启整条记录的逻辑日志记录, 产生大量wal, 性能也会受到影响.
    + > 必须开启wal level=logical, 需要重启, 同时会产生更多的wal日志
  * > pglogical不支持DDL的同步, Sequence的同步等.
  * > pglogical的使用门槛较高, 一般用户搞不定.

被锁住的大象(Postgres)，如何跟MySQL赛跑 https://mp.weixin.qq.com/s/s0NUrIq_o4Pj_RM_Qn7afQ

# 2024.05

Tom lane“糟老头子坏得很”可不是我说的 https://mp.weixin.qq.com/s/tbreZLw0FWwl80BBBnXTlA

第19期吐槽:从DuckDB导入到PG后膨胀了5倍，把存储销售乐坏了！什么情况? https://mp.weixin.qq.com/s/ag2rgVmPDudsEYwvu-Yi_g
- > 1、产品的问题点
  * > 一些时序场景数据外部数据文件导入数据库时占用空间膨胀,可能的原因包括，数据库的数据块有一些头信息，行也有头信息占用额外空间；或者使用了低于100%的填充率配置使得数据块的空间没有全部占满；还有一个重要的原因是外部文件可能是压缩的而数据库没有使用压缩，或者说行级别的main或toast压缩率是很低的。
  * > 相比于duckdb parquet, pg的存储空间消耗是其5.3倍. 详细可参考这篇文章的测试: [性能被DuckDB碾压,PG最应该反省哪几方面？](https://mp.weixin.qq.com/s/E3IkbSwW7IPu0D0UkdRhMQ)
  * > **对比列存储压缩, PG占用的空间是DuckDB的5.3倍**

第18期吐槽:都走索引了为什么还要回表访问?原来是索引里缺少了“灵魂” https://mp.weixin.qq.com/s/49_kHWi-BY2HFypPYqlVAg
- > 1、产品的问题点
  * > PG 索引中没有存储tuple版本信息, 即使仅查询索引包含的字段内容也可能需要回表进行判断.
- > 2、问题点背后涉及的技术原理
  * > ***PG 的索引没有TUPLE版本信息, 所以必须回表才能判断索引指向的tuple是否为dead tuple、是否对当前事务可见. 回表访问就意味着离散IO, 而且还占用更多的shared buffer资源***.
  * > 为了解决这个问题, PG引入VM file, 记录每个data block的状态, 当datablock为clean状态时, 则不需要回表, 但是也要访问VM才能判断datablock为clean状态, 所以离散IO次数是没有节省下来的, 节省的是IO覆盖范围, 毕竟VM里面2bit就表示1个data block(默认8KB).
- > 3、这个问题将影响哪些行业以及业务场景
  * > 对IO敏感的场景, 例如: 多对象的时序、时空场景. 对某个对象, 范围扫描大量记录, 需要回表, 产生大量离散IO. 例如: 查询某个车辆的轨迹数据, 将需要访问上万条离散记录. 查询某个用户在某个时间段的行为, 例如用户消费记录、话费、短视频浏览历史记录、视频历史观看记录、音频历史记录等等.
- > 4、会导致什么问题?
  * > 即使采用 include index , 叶子节点包含需要查询的所有数据, 如果需要回表判断, 离散IO依旧是问题(从索引到table或VM文件都是离散IO).
  * > 额外对table数据文件的回访, 导致浪费shared buffer.
  * > 在删除大量数据后 或者 更新大量数据后, 如果索引的垃圾版本未及时进行垃圾回收, 将导致命中这部分索引的Query性能急剧下降.
- > 5、业务上应该如何避免这个坑
  * > 尽量不要使用更新, 并且使用include index, 使得大部分数据块内的记录都是对所有会话可见, 所以不需要回表查询, 减少IO.
  * > 尽量不要使用更新, 并且使用多值类型存储多条点的聚合数据, 例如array或json, 使得单一对象的多条记录聚合为1条, 密集存储在少量block里面, 可以降低IO的数量.
  * > 按对象ID分区, 使用BRIN索引代替BTREE索引, 减少时序类数据的范围搜索IO.
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内核层解决, 降低回表次数, 除了vm file, 是否可以考虑引入索引支持版本更新? 因为本身写入、删除、更新时就需要更新对应的索引tuple, 索引支持记录多版本信息并没有增加更多的IO操作, 只是需要增加存储空间.
  * > ***或者使用undo这样的存储引擎, 因为是in-place update, 不存在索引多版本的问题***.

第17期吐槽:被DDL坑过的人不计其数！严重时引起雪崩，危害仅次于删库跑路！PG官方不支持online DDL确实后患无穷 https://mp.weixin.qq.com/s/-hEbABhI_hkQcwWPkqqnzQ
- > 1、产品的问题点
  * > PG 官方还不支持online DDL
- > 2、问题点背后涉及的技术原理
  * > DDL时需要加排他锁, 堵塞所有与被锁对象相关的操作, 包括select. ***而且PG是锁排队机制，即使当前DDL还没有获得排它锁，它其实已经会堵塞其他任何锁的请求***，再访问较频繁的表上操作的话容易引起大量等待甚至雪崩。
  * > 当然, ***PG很多DDL操作不需要table rewrite, 只需要改元数据, 例如加字段, 某些改字段长度的操作(具体见手册内alter table的语法介绍)***.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 几乎所有行业, 当需要对大表执行DDL(例如发布变更), 而且这个DDL需要table rewrite时.
  * > 即使没有table rewrite,如果这个表上面有其他已经持有锁的未结束长事务或query，那么一样会引起堵塞，也就是上面第二点里说的。
  * > 一些典型的场景是刷新统计数据，然后切换表名DDL、或者直接truncate 然后刷新数据，和逻辑备份或长的事务或查询冲突。
  * > 又比如修改字段类型、添加字段等。
- > 4、会导致什么问题?
  * > 当DDL需要table rewrite时. 那么需要长时间持有排他锁, 如果是个被频繁访问的表, 可能长时间影响业务, 甚至需要停业务来执行DDL.
  * > 即使DDL没有table rewrite,如果这个表上面有其他已经持有锁的未结束长事务或query，那么一样会引起堵塞，也就是上面第二点里说的。

第15期吐槽:没有全局临时表,除了难受还有哪些潜在危害？ https://mp.weixin.qq.com/s/7N35lF2bVQGWx9tcyqzThA
- > 1、产品的问题点
  * > PG 没有全局临时表
- > 2、问题点背后涉及的技术原理
  * > PG 的临时表是使用时创建, 结构定义在1个会话内有效.
  * > 每次创建临时表时, 需要在pg_class, pg_stats, pg_attribute等元数据中插入临时表的对象数据、统计信息数据、字段定义数据等. 在会话结束是再从这些元数据表中删除.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 频繁使用临时表的业务. 例如将业务逻辑放入数据库存储过程中, 使用临时表来存放一些中间计算结果.
- > 4、会导致什么问题?
  * > 元数据表、元数据表对应的索引膨胀.
  * > 使用不便, 每次使用临时表都需要重新创建.

性能被DuckDB碾压,PG最应该反省哪几方面？ https://mp.weixin.qq.com/s/E3IkbSwW7IPu0D0UkdRhMQ
- > 2、分析能力被全面碾压
  * > 一些用户在数据量不是特别大(例如TB级别时)会将postgres用于分析型业务, 因为pg 9.6开始引入了并行计算, 分析性能还可以, 并且它不像大型分析产品部署和管理起来那么复杂, 对机器要求也不高, 适合中小型数据量的分析. 但是现在性能被duckdb全面碾压了, 看本文后面的测试数据. 并且DuckDB使用起来还更加简单.
  * > 当然PG也有一些增强分析性能和数据融合能力的插件, 如parquet fdw, pg_lakehouse, duckdb fdw, columnar等插件.
  * > pg_lakehouse 理念很不错, 把PG从孤岛变成连接外部数据的桥梁, 结合pg_analytics实现分析(datafusion那套架构), 让用户只要使用Pg就可以处理TP和AP. oss、s3 都是云上使用最为广泛的产品, 为什么不是云数据库厂商先发做出这块能力(我是说要有好的体验)的呢?
- > 3、PG扩展接口最近发展真的是太慢了, 导致包括以上在内的插件从性能到体验都离产品化有一段距离. 再这样发展下去, extension也很快会被碾压. 最近DuckDB出的extension都非常的赞, 体验杠杠的.举几个例子:
  * > fdw接口都出来多少年了, 下推还行, 但是事务支持、数据传输效率、cache plan等都有改进空间, 所以通过fdw来读写oss里面的parquet数据效率还有很大提升空间.
  * > tam(table access method)也有很多需要完善的地方, 所以你看从PG 12发布tam后到现在都6年了, 一个真正能打（生产大量应用）的三方 table am 都没有. 甚至17版本发布之前上线的好几个tam扩展能力, 又被tom lane打回. 参考: 《PostgreSQL 17 到底值不值得期待? 看完这篇就明白了.》 tom lane一边拒绝 tam, 自己crunchydata databridge 确大张旗鼓的改进tam, 提升s3等云对象存储访问效率和体验. 参考: 《什么? PostgreSQL大佬tom lane公司crunchydata“模仿”DuckDB创意?》
  * > 另外是wire protocol兼容性扩展能力, aws负责babelfish(协议层兼容sql server)哥们提交的增加wire protocol hook的Patch, 死活不让进Master分支, 导致扩展Pg 语法兼容性特别麻烦.
- > 开始PK
- > 小结
  ```console
  DuckDB+parquet tpch(sf=20) 22条query 总耗时 48.615 秒
  PostgreSQL+duckdb_fdw+parquet+query view (相当于全部pushdown到DuckDB) tpch(sf=20) 22条query 总耗时 201.25 秒
  PostgreSQL本地行存表(包括索引) tpch(sf=20) 22条query 总耗时 2790.94 秒
  ```

权限管控，还可以再简单点 https://mp.weixin.qq.com/s/DabBsutelkiLKYsE9PrbIg

第13期吐槽:十个中年人有九个发福的，数据库用久了也会变胖！这一期吐槽PG膨胀收缩之痛，tom lane啊您为啥不根治膨胀呢？ https://mp.weixin.qq.com/s/rNzkNPFPL9Kg6RP01sVkZQ
- > 1、产品的问题点
  * > 当表膨胀后普通的vacuum 操作无法回收已经占用的磁盘空间, (仅末尾空块可从磁盘回收).
  * > 使用pg_repack(增量复制+数据文件filemap切换, 短暂锁表)或vacuum full(要锁全表, 影响业务)回收磁盘占用的空间都需要额外的磁盘来临时存储重组后的数据.
- > 2、问题点背后涉及的技术原理
  * > ***普通的vacuum只能truncate数据文件末尾的空block, 所以我们可以将末尾的tuple移动到前面, 从而从磁盘回收末尾的block***.
  * > 为什么只能truncate数据文件末尾的空block?
    + > 因为非末尾的block被清掉之后, 这个存储在被清理的block之后的block里面的记录的寻址会发生变化, 例如第二个数据块回收掉, 那么2号数据块后面的数据块的编号都需要减1, 而索引的ctid指向的是原来的编号, 因此会导致索引内容错误. 当然, 我们可以在meta信息里增加1个bitmap文件存储真空块(已回收的中间blockid, 寻址时通过这个数据再进行block定位), 但是会增加寻址的复杂度, 性能可能下降. 而且PG使用的是文件系统管理, 可能还需要文件系统支持这样的"空间压缩技术".

第10期吐槽:说删库跑路的都是骗子,千万别信,他们有的宝贝你可能没有! https://mp.weixin.qq.com/s/L9c-xmYgTXzutSpREtTUuA
- > 1、产品的问题点
  * > 没有Query级别的闪回功能, 当发生DML误操作后, 恢复数据比较困难.
- > 2、问题点背后涉及的技术原理
  * > flashback query属于查询“数据”在过去某个时刻的状态, 实现方法举例:
    + > 需要有旧的tuple版本, 以及事务提交或回滚状态, 事务结束时间的信息.这种实现方法需要保留旧版本和事务结束时间, 可能导致UNDO数据膨胀.
    + > 或: 采用快照, 通过快照+WAL回放的形式回到过去状态.这种实现方法需要支持快照, 可能导致额外的copy on write开销
    + > 最好仅仅针对重点表开启闪回功能.

第9期吐槽:抓狂,最先进的开源数据库上万连接就扛不住了? https://mp.weixin.qq.com/s/02x3eFzB1XnWW78ADclkrg
- > 1、产品的问题点
  * > 大量连接+大量写小事务 性能差
- > 2、问题点背后涉及的技术原理
  * > PG判定事务可见性依赖事务快照, 结构为:`ProcArray`, ***事务启动时、RC事务隔离级别的Statement执行开始时都要加`ProcArray共享锁`, 写操作的事务结束时需要加`ProcArray排他锁`***. 高并发写操作发生时容易产生`ProcArray排他锁`冲突. ***虽然procarray是有hash分区每次只锁映射的分区来降低排他锁冲突***, 但是连接过多的情况下冲突依旧明显.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 读写频繁、高并发(大量连接, 通常指超过`5000`个连接)小事务的业务. 例如2C的SaaS类场景.
- > 4、会导致什么问题?
  * > 高并发写操作发生时容易产生ProcArray排他锁冲突. 性能下降. `上万连接的高并发写操作`性能可能降低到`1000 tps`以内.
- > 5、业务上应该如何避免这个坑
  * > 使用连接池, 降低总连接数.
  * > 如果应用程序本身不具备连接池的能力, 使用pgbouncer这类中间连接池
- > 6、业务上避免这个坑牺牲了什么, 会引入什么新的问题
  * > 管理更加复杂
  * > pgbouncer引入后, 必须是要statement或transaction level连接池, 从而无法使用prepared statement, 导致query parse,rewrite,plan的开销增加.
  * > 更新: 《pgbouncer 1.21 开始支持 prepared statement in 事务模式》
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内置线程连接池
  * > 对事务快照进行 CSN 或 CTS 改造 https://github.com/alibaba/PolarDB-for-PostgreSQL/blob/master/doc/polardb/cts.md
  * > 内置连接池插件: https://github.com/nextgres/nextgres-idcp
  * > 使用PolarDB, 内置shared server, 可以抵挡上万高并发小事务性能不降. 参考github文章: 《开源PolarDB|PostgreSQL 应用开发者&DBA 公开课 - 5.5 PolarDB开源版本必学特性 - PolarDB 特性解读与体验》
  * > postgrespro也发表过内置连接池的功能, 参考github文章: 《PostgresPro buildin pool(内置连接池)版本 原理与测试》 而且这个patch 在9年之前的PG 9.6版本发布时就有提交, 可惜一直没有被社区接收, 又要再多加一条吐槽, 是谁在阻止好的功能合并入社区版本? 拉出来鞭尸一百遍.

第8期吐槽:高并发短连接性能怎么这么差? https://mp.weixin.qq.com/s/wIQbhl4Ia7stxO6IlKQM6Q
- > 1、产品的问题点
  * > 高并发的短连接性能差劲
- > 2、问题点背后涉及的技术原理
  * > ***`短连接`是指每次发起SQL请求时新建数据库连接, SQL请求结束后断开数据库连接的情况. 由于PG是进程模型, 每次发起会话时需要fork process, memcpy等动作. 每秒可以新建的进程数比较有限***.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 没有连接池的高并发业务
- > 4、会导致什么问题?
  * > 性能极差, 每秒新建连接数可能不到2000
- > 5、业务上应该如何避免这个坑
  * > 使用连接池
  * > 如果应用程序本身不具备连接池的能力, 使用pgbouncer这类中间连接池

第7期吐槽:今年才等来slot failover，附上海DBA招聘信息 https://mp.weixin.qq.com/s/ujRxjJSgyWU9jbOl2RGuOw

第5期吐槽:经常OOM?吃内存元凶找到了:元数据缓存居然不能共享 https://mp.weixin.qq.com/s/zy158rSc0t7gDF2bNCpCIA
- > 1、产品的问题点
  * > meta cache (rel/catalog cache), plan cache是每个会话私有的内存.
- > 2、问题点背后涉及的技术原理
  * > 正常的SQL执行过程包括sql parse, rewrite, plan, exec等几个过程, parse, rewrite, plan都比较耗费cpu, 在OLTP的短平快场景, 使用prepared statement可以避免每次调用都需要parse,rewrite,plan, 使用plan cache直接进入exec阶段(特定情况下的custom plan除外).
  * > 同时为了处理sql parse, rewrite, plan等, 数据库还需要一些meta cache, 例如访问过的表结构, 索引, 视图等定义.
  * > plan cache和meta cache都是会话进程私有的.
  * > PG 为每个会话分配一个backend process.
- > 4、会导致什么问题?
  * > SaaS行业, 每个B端用户一套schema, 表超级多, 一个会话在整个生命周期内可能访问很多的数据库对象, 产生很多的plan cache、relcache, 单个会话对应的backend process占用大量内存. 进程多的话会导致内存消耗巨大, 导致OOM.
  * > 分区超多的, 而且使用长连接和绑定变量. 频繁更新的C端业务系统通常有这个特性, 例如共享单车, 单车数量多, 用户多, 需要通过分区提高垃圾回收和freeze的效率. 导致的问题同上.
  * > 微服务, 服务超级多, 导致与数据库的连接过多. 进而导致以上类似问题.
- > 5、业务上应该如何避免这个坑
  * > 控制每个会话的生命周期, 从而避免长时间touch过多的relation, 导致内存爆增.
  * > 控制总连接数, 从而降低所有会话导致的整体内存使用.
  * > 使用高版本PG(大版本在逐渐优化)或pg_pathman, 避免即使只访问某个分区, 在plan过程依旧需要touch所有分区表.
  * > 使用pgbouncer连接池, 控制总连接数.
- > 6、业务上避免这个坑牺牲了什么, 会引入什么新的问题
  * > 增加了复杂度, 很多初次使用PG的小伙伴不知道.
  * > 微服务很多的时候, 每个微服务至上的1个连接吧, 所以控制总连接数无解.
  * > 使用pgbouncer控制总连接的话必须使用statement或transaction level, ***这样的话就不能使用prepared statement***, 因为下次发起exec时可能已经不是之前那个backend process了.
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内置线程池
  * > global cache (rel catalog caches, plan)
    + > pg_backend_memory_contexts 查看内存上下文

# 2024.03

Oracle的RAC神话被打破了? https://mp.weixin.qq.com/s/AYAdccNbJI_7KatmP7UGsA
- > 因此, 传统的共享存储架构, 主库在用的时候, 备库是不能启动的。仅仅用户HA.

# 2024.01

数据库实验手册系列:12 如何快速模拟“海量逼真”测试数据 https://mp.weixin.qq.com/s/5rIqMv1WnL5x2QELQjsR2g
