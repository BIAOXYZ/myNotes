
# 2024.05

第10期吐槽:说删库跑路的都是骗子,千万别信,他们有的宝贝你可能没有! https://mp.weixin.qq.com/s/L9c-xmYgTXzutSpREtTUuA
- > 1、产品的问题点
  * > 没有Query级别的闪回功能, 当发生DML误操作后, 恢复数据比较困难.
- > 2、问题点背后涉及的技术原理
  * > flashback query属于查询“数据”在过去某个时刻的状态, 实现方法举例:
    + > 需要有旧的tuple版本, 以及事务提交或回滚状态, 事务结束时间的信息.这种实现方法需要保留旧版本和事务结束时间, 可能导致UNDO数据膨胀.
    + > 或: 采用快照, 通过快照+WAL回放的形式回到过去状态.这种实现方法需要支持快照, 可能导致额外的copy on write开销
    + > 最好仅仅针对重点表开启闪回功能.

第9期吐槽:抓狂,最先进的开源数据库上万连接就扛不住了? https://mp.weixin.qq.com/s/02x3eFzB1XnWW78ADclkrg
- > 1、产品的问题点
  * > 大量连接+大量写小事务 性能差
- > 2、问题点背后涉及的技术原理
  * > PG判定事务可见性依赖事务快照, 结构为:`ProcArray`, ***事务启动时、RC事务隔离级别的Statement执行开始时都要加`ProcArray共享锁`, 写操作的事务结束时需要加`ProcArray排他锁`***. 高并发写操作发生时容易产生`ProcArray排他锁`冲突. ***虽然procarray是有hash分区每次只锁映射的分区来降低排他锁冲突***, 但是连接过多的情况下冲突依旧明显.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 读写频繁、高并发(大量连接, 通常指超过`5000`个连接)小事务的业务. 例如2C的SaaS类场景.
- > 4、会导致什么问题?
  * > 高并发写操作发生时容易产生ProcArray排他锁冲突. 性能下降. `上万连接的高并发写操作`性能可能降低到`1000 tps`以内.
- > 5、业务上应该如何避免这个坑
  * > 使用连接池, 降低总连接数.
  * > 如果应用程序本身不具备连接池的能力, 使用pgbouncer这类中间连接池
- > 6、业务上避免这个坑牺牲了什么, 会引入什么新的问题
  * > 管理更加复杂
  * > pgbouncer引入后, 必须是要statement或transaction level连接池, 从而无法使用prepared statement, 导致query parse,rewrite,plan的开销增加.
  * > 更新: 《pgbouncer 1.21 开始支持 prepared statement in 事务模式》
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内置线程连接池
  * > 对事务快照进行 CSN 或 CTS 改造 https://github.com/alibaba/PolarDB-for-PostgreSQL/blob/master/doc/polardb/cts.md
  * > 内置连接池插件: https://github.com/nextgres/nextgres-idcp
  * > 使用PolarDB, 内置shared server, 可以抵挡上万高并发小事务性能不降. 参考github文章: 《开源PolarDB|PostgreSQL 应用开发者&DBA 公开课 - 5.5 PolarDB开源版本必学特性 - PolarDB 特性解读与体验》
  * > postgrespro也发表过内置连接池的功能, 参考github文章: 《PostgresPro buildin pool(内置连接池)版本 原理与测试》 而且这个patch 在9年之前的PG 9.6版本发布时就有提交, 可惜一直没有被社区接收, 又要再多加一条吐槽, 是谁在阻止好的功能合并入社区版本? 拉出来鞭尸一百遍.

第8期吐槽:高并发短连接性能怎么这么差? https://mp.weixin.qq.com/s/wIQbhl4Ia7stxO6IlKQM6Q
- > 1、产品的问题点
  * > 高并发的短连接性能差劲
- > 2、问题点背后涉及的技术原理
  * > ***`短连接`是指每次发起SQL请求时新建数据库连接, SQL请求结束后断开数据库连接的情况. 由于PG是进程模型, 每次发起会话时需要fork process, memcpy等动作. 每秒可以新建的进程数比较有限***.
- > 3、这个问题将影响哪些行业以及业务场景
  * > 没有连接池的高并发业务
- > 4、会导致什么问题?
  * > 性能极差, 每秒新建连接数可能不到2000
- > 5、业务上应该如何避免这个坑
  * > 使用连接池
  * > 如果应用程序本身不具备连接池的能力, 使用pgbouncer这类中间连接池

第5期吐槽:经常OOM?吃内存元凶找到了:元数据缓存居然不能共享 https://mp.weixin.qq.com/s/zy158rSc0t7gDF2bNCpCIA
- > 1、产品的问题点
  * > meta cache (rel/catalog cache), plan cache是每个会话私有的内存.
- > 2、问题点背后涉及的技术原理
  * > 正常的SQL执行过程包括sql parse, rewrite, plan, exec等几个过程, parse, rewrite, plan都比较耗费cpu, 在OLTP的短平快场景, 使用prepared statement可以避免每次调用都需要parse,rewrite,plan, 使用plan cache直接进入exec阶段(特定情况下的custom plan除外).
  * > 同时为了处理sql parse, rewrite, plan等, 数据库还需要一些meta cache, 例如访问过的表结构, 索引, 视图等定义.
  * > plan cache和meta cache都是会话进程私有的.
  * > PG 为每个会话分配一个backend process.
- > 4、会导致什么问题?
  * > SaaS行业, 每个B端用户一套schema, 表超级多, 一个会话在整个生命周期内可能访问很多的数据库对象, 产生很多的plan cache、relcache, 单个会话对应的backend process占用大量内存. 进程多的话会导致内存消耗巨大, 导致OOM.
  * > 分区超多的, 而且使用长连接和绑定变量. 频繁更新的C端业务系统通常有这个特性, 例如共享单车, 单车数量多, 用户多, 需要通过分区提高垃圾回收和freeze的效率. 导致的问题同上.
  * > 微服务, 服务超级多, 导致与数据库的连接过多. 进而导致以上类似问题.
- > 5、业务上应该如何避免这个坑
  * > 控制每个会话的生命周期, 从而避免长时间touch过多的relation, 导致内存爆增.
  * > 控制总连接数, 从而降低所有会话导致的整体内存使用.
  * > 使用高版本PG(大版本在逐渐优化)或pg_pathman, 避免即使只访问某个分区, 在plan过程依旧需要touch所有分区表.
  * > 使用pgbouncer连接池, 控制总连接数.
- > 6、业务上避免这个坑牺牲了什么, 会引入什么新的问题
  * > 增加了复杂度, 很多初次使用PG的小伙伴不知道.
  * > 微服务很多的时候, 每个微服务至上的1个连接吧, 所以控制总连接数无解.
  * > 使用pgbouncer控制总连接的话必须使用statement或transaction level, ***这样的话就不能使用prepared statement***, 因为下次发起exec时可能已经不是之前那个backend process了.
- > 7、数据库未来产品迭代如何修复这个坑
  * > 内置线程池
  * > global cache (rel catalog caches, plan)
    + > pg_backend_memory_contexts 查看内存上下文

# 2024.03

Oracle的RAC神话被打破了? https://mp.weixin.qq.com/s/AYAdccNbJI_7KatmP7UGsA
- > 因此, 传统的共享存储架构, 主库在用的时候, 备库是不能启动的。仅仅用户HA.

# 2024.01

数据库实验手册系列:12 如何快速模拟“海量逼真”测试数据 https://mp.weixin.qq.com/s/5rIqMv1WnL5x2QELQjsR2g
