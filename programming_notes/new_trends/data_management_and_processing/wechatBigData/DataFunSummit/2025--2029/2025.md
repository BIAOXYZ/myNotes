
# 03

Apache Spark SQL 优化器 https://mp.weixin.qq.com/s/9qfJwwDEug6v1xHKzWl-sQ
- > 导读: 在《Spark SQL 解析层优化》、《Spark SQL 分析层优化》以及《Spark SQL 表达式优化》三次分享中，分别介绍了从 SQL 解析、 SQL 分析以及 SQL 表达式三个角度， 通过优化框架、提炼代码、优化数据结构等方法来进行优化。以上优化更多的是从代码设计方向的优化，是广义上的 SQL 优化。业内普遍讨论的 SQL 优化则是 SQL 优化器和执行计划的优化。本文将剖析 Spark SQL 优化器原理，并在此基础上介绍三种较常见的优化思路。这些优化器的执行计划优化范式是可以复用的。
- > 主要内容包括以下几大部分：1. Spark SQL 优化器原理 2. 下推优化思路及代码实现 3. 算子消除与算子合并 4. 表达式消除与表达式替换 5. 总结
- > **01 Spark SQL 优化器原理**
  * > 图中的 Optimizer，就是 Spark SQL 优化器。***Optimizer 是通用名字，实际上有不同的实现***。优化器与之前的分析层类似，需要用到元数据信息协助它做优化。
  * > 无论是分析器还是优化器，都是规则执行器的具体实现。优化器中的优化规则与分析器规则有很大的不同，***分析器里主要是元数据的绑定从而定位数据的信息；而优化器在分析器基础上，借助分析结果做进一步优化***。在 Spark 3.0.0 之前，Spark 优化器是 Spark Optimizer 组件。Spark 3.0.0 之后引入了 `AQE（Adaptive Query Execution）`，它实际上也是规则执行器的实现，复用了很多原来的规则，并额外增加了一些新的优化规则，如小分区合并、大分区拆分、数据倾斜的处理等。但总体上从技术角度而言，代码设计同之前的优化器都是类似的。下面主要围绕经典的优化器进行介绍。

ClickHouse 在实时 OLAP 的创新与突破 https://mp.weixin.qq.com/s/GGaFl3rsRaK4bpV9pIT9hA
- > **01 ClickHouse 是什么**
  * > **2. ClickHouse 的特点**
    + > 在线查询
      - > ClickHouse 从开源到现在一直都是以查询性能著称，***面对大数据量的复杂查询可以做到实时应答，并且无需进行预处理或预聚合***，所以非常适用于对性能要求高的场景。
    + > 高性能
      - > ***ClickHouse 的设计思路就是先把单机性能做到极致，再去考虑扩展性***。所以其单机性能非常强大。***<ins>主要体现在其 `MergeTree 引擎`以及 `Pipeline 执行引擎`，都比较贴合现代硬件的特性</ins>***，可以提供非常好的查询性能、写入性能以及存储压缩率。
    + > **Everything is table**
      - > ![](https://mmbiz.qpic.cn/sz_mmbiz_png/EBaibcQicPxgx96XPPq4S2xnBe7dICibo9gozjK0pnqMWXQeib3y1dm6GTnZRyBCknmq6W9lLDcLZxIhcYeBGhsialw/640)
      - > 在 ClickHouse 中，表被作为一级公民，ClickHouse 对用户提供的各种数据源都是通过抽象成 Table Engine 的方式来实现的。包括我们对外部资源的访问，比如 S3、HDFS 或者本地文件，都可以通过 Storage Engine 去访问。同时表引擎的接口抽象是非常简单的，通常只需要访问一些外部资源，可以很快地扩展几个表引擎的接口，快速实现简单的表引擎。
      - > 非常重要的一点是，ClickHouse 内置了用于原生数据存储的 MergeTree Family。MergeTree Family 是默认的存储引擎，下面按照不同的功能特性分成了多种不同的表引擎，比如常见的没有任何其它功能特性，只是做数据存储的 MergeTree，用于去重的 Replacing MergeTree，还有做聚合的 Aggregating MergeTree 以及 Summing MergeTree。
      - > 另外一个重要的部分就是 Replicated MergeTree Family，其特性基本是与 MergeTree Family 对齐的。主要区别就是 Replicated MergeTree 支持了高可用的能力，社区版的 ClickHouse 就是基于 Replicated MergeTree 实现的高可用的副本能力。
  * > **3. 性能有多快**
    + > ![](https://mmbiz.qpic.cn/sz_mmbiz_png/EBaibcQicPxgx96XPPq4S2xnBe7dICibo9gZY7nVAj3cbaoOfG5WmLqr4gpCkPOVVxnZLKsKaK8eT4DyTd70T8gSA/640)
    + > ClickHouse 从开源到现在，始终保持着领先的性能，包括查询和写入。上图左下角是官方 benchmark 地址，欢迎大家查看，很多其它 OLAP 引擎也基于这个 benchmark 去测评其引擎能力。这个 benchmark 里包含了我们从大量用户 case 里总结出来的测试用例以及各种数据集，上面有完备的测试步骤，包括这个数据集怎么去运行测试表脚本，可以很好地复现测试结果。
    + > 从数据中可以看到，ClickHouse 对比传统数据库有着数百倍甚至千倍的性能提升，相比于最近主流的 AP 系统，其性能也处于领先位置。
  * > **4. ClickHouse 的商业化**
    + > 上图展示了自建和 Cloud 的一些对比。首先是运维成本不同，ClickHouse Cloud 开箱即用的能力，云托管可以节省大量的运维成本以及调参成本。***第二是在云上社区版的`存算一体架构`切换成了全新的`存算分离架构`***，同时基于存算分离架构引入了很多新的特性，后面将会着重介绍。
- > **02 ClickHouse 在实时 OLAP 领域的创新**
  * > **1. 实时 OLAP 的一些特点和挑战**
    + > 在实时 OLAP 领域，通常用户会有非常高的写入吞吐。我们云上的一些大客户场景会达到`千万行每秒`的数据量级。同时他们在维持高写入吞吐的同时，还需要非常低的数据可见性，比如秒级的延迟。
    + > 这样的需求对于大多数 OLAP 数据库，包括 ClickHouse 都是有一定挑战性的。***因为 OLAP 数据库的特点是倾向于攒批写入***，它对于 OLAP 数据库的性能是最好的。但是在这种情况下，***我们为了维持数据的实时可见，不可能维持特别大的时间窗口去进行攒批***。所以在实时的场景下，***我们就需要在几十秒甚至秒级别频繁地高并发地写入数据***。
    + > ***在这种情况下每次写入 ClickHouse 里都会产生 Parts 文件***，在写入并发量很高的时候，每秒在 ClickHouse 里产生的文件数就会非常多，会有非常多的小文件。如果 ClickHouse 的 merge 性能没有跟上，会造成小文件在存储里的堆积。这样会造成一些问题，比如常见的 `TOO_MANY_PARTS` 错误。同时也会因为小文件的堆积造成写入的 delay insert 以及查询的变慢。
    + > 要解决这一问题，最简单的想法就是让更多的工作节点去承担 merge 负载，以提高 merge 速率。因为核心问题就在于 merge 不及时，所以如果能通过横向扩展资源，让 merge 速率能够快速跟上写入速度，就能够将 Parts 数维持在动态平衡的数量，这样就不会有问题了。
    + > ***实际上线性扩展一直是在分布式领域的追求，尤其是对于数据库这种有状态的服务，但这是有一定挑战性的***。甚至很多时候节点加多了，反而会因为各种各样的资源占用，以及分布式协调相关的开销，导致性能不但没有随着资源的增加而线性增长，反而是减少了。
  * > **2. 基于 `Share Nothing` 架构的 `ReplicatedMergetree*` 引擎**
    + > 社区版的 ClickHouse 分布式扩展的方式有两种，一种是`分片`，一种是`加副本`。首先分片就是把数据水平切分，加副本就是对一份被切分后的数据，让多个节点存相同的数据，能够容灾，也可以同时提供对这一份数据的查询。
    + > 但是实时写入经常会出现流量的变化，随着业务的增长流量可能会不断上升，这种情况下就要考虑去扩 shard。扩 shard 有两种场景，一种是之前分 shard 时分的比较多，需要把一些 shard 从老的节点 rebalance 到新的节点，这需要比较重的数据搬迁。第二种就是之前的 shard 分的还不够多，甚至可能没有办法均分到新节点上，这样就更糟糕，还需要再重新调整数据的 shard key 重新对数据再进行一次切分，才能把它 rebalance 掉。总而言之，这两种情况都会带来非常长的扩容周期，在扩容期间业务会有很强的感知，可能还需要业务做数据回溯等工作，并且新扩容的节点是没有办法提供服务的，也就无法分担集群负载。这就是通过新的分片的方式水平扩展存在的问题。
    + > 如果通过添加副本的方式水平扩展会怎样呢？首先因为副本都享有同一份数据，所以通过添加副本的方式进行水平扩展对于查询来说是有效的，因为可以让针对这一份数据的查询分散在不同的副本上，但是对于 merge 压力是没有办法线性增长与分摊的。在后面会介绍一个例子，其中可以很明显地看到，增加 replica， merge 性能并没有线性增长，甚至在某个阶段还会退化。
  * > **3. 基于云原生 Serverless 架构的 `shardMergetree*` 引擎**
    + > 为了解决这个问题，ClickHouse 在云上进行了一轮架构升级。推出了云原生的 Serverless 的 MergeTree Engine，叫做 SharedMergeTree。对比 ReplicatedMergeTree 最大的区别就是它像 everything 的架构，把原来的存算一体架构变成了存算分离的架构，所有的数据持久化到统一的存储中。比如这里指的是 Object Store，那就可以是 AWS 的 S3 或者阿里云的 OSS 等等。同时各个节点的本地盘也可以被用于热数据的缓存，同时也支持数据写入的时候双写本地盘或远端存储。
    + > ***该架构挪去了副本的概念***，因为副本是用于容灾的，在这种架构下每个节点都可以看到全量的数据。所以当我们使用一张分布式表的时候，使用一张分布式的 shard MergeTree，和使用单节点的 MergeTree 表是一样的。抹除了 shard 必须多副本的限制之后，带来的好处就是在某些场景下，查询的 QPS 并不需要多副本去承载，就可以把副本节点抹掉，相当于抹掉了一半的存储和计算资源。
    + > 数据同步以及水平扩容方面，因为各个节点通过 Keeper 同步新写入的数据，但是在初次同步的时候，只需要同步元数据，就可以让新节点开始提供服务。当新的数据被写入到远端存储之后，会在 Keeper 上被提交，其它节点也会通过 watch 之类的同步方式感知到有新的 part 被提交了，那么其它节点就会从 Keeper 以及对象存储中将 part 的元数据同步过来，这时新写入的数据就可以对所有节点提供服务了。此时我们还不需要同步真实的数据，同步元数据是非常快的过程，这个节点就已经可以基于元数据尝试对这个新写入的 part 调度进行 merge 了。
    + > 这一架构的设计初衷是为了改进 ClickHouse 社区版在分布式上的一些不足，所以是云原生 Serverless 的设计，并且是面向大服务器组成集群设计的，上百个节点都是 OK 的。在这个设计下，***摒弃了 shard 切分的概念***，我们也可以很容易地横向扩展节点。比如节点被加入到集群中，只需要完成基本的元数据同步，这一过程通常很快，一般在云上仅需十几秒，然后就可以去承载 merge 的查询。同时它也可以按照一致性哈希，将少量需要的原始数据同步到新节点的本地热盘，更好地对查询进行加速。不像在社区版扩 shard，需要对整体的数据进行 rebalance，完成 rebalance 之后才能开始分担集群的压力。
    + > 接下来介绍一下之前提到的一个基准测试，即 SharedMergeTree 和 ReplicatedMergeTree 在 merge 场景的水平扩展性能测试。可以看到基准测试就是看集群能多快地把相同数量的一张表 merge 到固定的 part。这里测试了两种情况，一个是 3000parts，一个是 300parts。merge 时间可以体现出 merge 性能，我们在更短的时间内把 part merge 掉，就说明 merge 性能更好。
    + > `80` 是 ReplicatedMergeTree 可能的极限值，***在 `80` 节点的时候可以看到 `ReplicatedMergeTree` 已经无法正常工作***；而 `SharedMergeTree` 仍可以很好地进行工作，并保持近线性增长的 merge 性能。
    + > 所以基于云原生 Serverless 架构的 SharedMergeTree，我们就可以通过横向扩展来提升 merge 性能，以应对实时流量突增的情况。
  * > **5. 社区版和企业版的对比**
    + > ![](https://mmbiz.qpic.cn/sz_mmbiz_png/EBaibcQicPxgx96XPPq4S2xnBe7dICibo9gOLzpNZA8VNETeVS801U4GotAt73NO05muib4a7chCJvyhnr0nKMwong/640)
- > **03 Q&A**
  * > **Q1：ClickHouse 下一步迭代的重点方向是什么？**
    + > A1：ClickHouse 每年会有 top 5 的 feature 介绍，这里介绍几个大家可能感兴趣的点。首先，对于数据湖生态的支持的丰富程度，后续会有进一步提升。此外还有针对全文索引的增强和重构，以及把它 GA 掉。这是两个比较大的改进点。
    + > 另外，Serverless SharedMergeTree 架构虽然已经处于 GA 状态，但实际上我们还是会根据云上用户 case 中碰到的一些场景不断迭代一些新的特性，比如后续可能会支持分布式缓存，以及支持 workload 多工作组去做读写分离等。
  * > **Q2：存算分离的这些能力，后续在社区版里面也会提供吗？**
    + > A2：目前存算分离的能力主要依赖于几个重要组件，包括 Keeper、Object store，以及 storage polics，这些在社区版中都是已经包含了的。***唯一闭源的就是 `SharedMergeTree`，其它的基本上在社区版中都有***。

# 02

Spark 向量化加速引擎 Blaze 的技术实现与应用案例 https://mp.weixin.qq.com/s/K4s-D7ZDPI1INByRq3PqzA

大模型内容风控--跨模态通用视觉内容安全审核技术 https://mp.weixin.qq.com/s/q776Ma0VV0vgtKXKQiiwOw

从数据库视角看 NL2SQL https://mp.weixin.qq.com/s/yMHxk5lDBiAh4ZKjf7N5KQ

# 01

优化 Spark SQL 执行层的两种关键策略！ https://mp.weixin.qq.com/s/9JsakURon9od9240StkfYA
